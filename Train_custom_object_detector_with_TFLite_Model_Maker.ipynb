{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vvAObmTqglq"
      },
      "source": [
        "### Install the required packages\n",
        "Start by installing the required packages, including the Model Maker package from the [GitHub repo](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker) and the pycocotools library you'll use for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhl8lqVamEty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2238f583-6a61-44b0-9642-85e69dcd1d5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 616 kB 7.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 69.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 87 kB 7.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 51.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 234 kB 72.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 59.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 840 kB 60.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 63.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 58.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 74.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 55.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 62.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47.7 MB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 11.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 69.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 1.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 211 kB 56.5 MB/s \n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q --use-deprecated=legacy-resolver tflite-model-maker\n",
        "!pip install -q pycocotools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6lRhVK9Q_0U"
      },
      "source": [
        "Import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtxiUeZEiXpt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import ExportFormat\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRd13bfetO7B"
      },
      "source": [
        "### Prepare the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoYVwAzQKY75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e9b68c-372c-4a8c-8390-79f7c3b9fb9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 74 kB 2.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 11.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 150 kB 43.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 113 kB 50.0 MB/s \n",
            "\u001b[?25h  Building wheel for kaggle-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lxml (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for lxml\u001b[0m\n",
            "\u001b[?25h  Building wheel for PrettyTable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "    Running setup.py install for lxml ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-ffma1xl8/lxml_2bd3c30a0b384808ad1eba09f9432796/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-ffma1xl8/lxml_2bd3c30a0b384808ad1eba09f9432796/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-qq66s0ac/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/lxml Check the logs for full command output.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install Kaggle API\n",
        "!pip install -q kaggle\n",
        "!pip install -q kaggle-cli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFKTF7qwKf7a"
      },
      "outputs": [],
      "source": [
        "# only for google colab\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"<username>\" \n",
        "os.environ['KAGGLE_KEY'] = \"<key>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfR7lTWrKhY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adad1646-bb52-4ca7-d43f-ffac892976b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "401 - Unauthorized\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d tannergi/microcontroller-detection --unzip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5M8iuydhVae"
      },
      "source": [
        "## Train your object detection model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xushUyZXqP59"
      },
      "source": [
        "There are six steps to training an object detection model:\n",
        "\n",
        "**Step 1. Choose an object detection model archiecture.**\n",
        "\n",
        "This tutorial uses the EfficientDet-Lite2 model. EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture. \n",
        "\n",
        "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
        "\n",
        "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
        "|--------------------|-----------|---------------|----------------------|\n",
        "| EfficientDet-Lite0 | 4.4       | 37            | 25.69%               |\n",
        "| EfficientDet-Lite1 | 5.8       | 49            | 30.55%               |\n",
        "| EfficientDet-Lite2 | 7.2       | 69            | 33.97%               |\n",
        "| EfficientDet-Lite3 | 11.4      | 116           | 37.70%               |\n",
        "| EfficientDet-Lite4 | 19.9      | 260           | 41.96%               |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtdZ-JDwMimd"
      },
      "outputs": [],
      "source": [
        "spec = model_spec.get('efficientdet_lite3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5U-A3tw6Y27"
      },
      "source": [
        "**Step 2. Load the dataset.**\n",
        "\n",
        "Model Maker can load data from multiple different formats, including [CSV format](https://cloud.google.com/vision/automl/object-detection/docs/csv-format) and [Pascal VOC](https://towardsdatascience.com/coco-data-format-for-object-detection-a4c5eaf518c5). For more information, check out the [`tflite_model_maker.object_detector.DataLoader` documentation](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/DataLoader)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7wwyI2gLKns"
      },
      "outputs": [],
      "source": [
        "train_data = object_detector.DataLoader.from_pascal_voc('/content/train', '/content/train', ['drone'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_nGbWyKMANB"
      },
      "outputs": [],
      "source": [
        "validation_data = object_detector.DataLoader.from_pascal_voc('/content/test', '/content/test', ['drone'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uZkLR6N6gDR"
      },
      "source": [
        "**Step 3. Train the TensorFlow model with the training data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwlYdTcg63xy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3debc636-52a3-483f-9982-dc589f6e07c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "96/96 [==============================] - 125s 840ms/step - det_loss: 0.8875 - cls_loss: 0.5472 - box_loss: 0.0068 - reg_l2_loss: 0.0926 - loss: 0.9802 - learning_rate: 0.0090 - gradient_norm: 4.1888 - val_det_loss: 0.6226 - val_cls_loss: 0.3955 - val_box_loss: 0.0045 - val_reg_l2_loss: 0.0928 - val_loss: 0.7155\n",
            "Epoch 2/40\n",
            "96/96 [==============================] - 77s 807ms/step - det_loss: 0.4537 - cls_loss: 0.2640 - box_loss: 0.0038 - reg_l2_loss: 0.0930 - loss: 0.5467 - learning_rate: 0.0100 - gradient_norm: 4.2147 - val_det_loss: 0.5766 - val_cls_loss: 0.3009 - val_box_loss: 0.0055 - val_reg_l2_loss: 0.0931 - val_loss: 0.6698\n",
            "Epoch 3/40\n",
            "96/96 [==============================] - 77s 799ms/step - det_loss: 0.4022 - cls_loss: 0.2375 - box_loss: 0.0033 - reg_l2_loss: 0.0933 - loss: 0.4955 - learning_rate: 0.0099 - gradient_norm: 4.5116 - val_det_loss: 0.5178 - val_cls_loss: 0.3432 - val_box_loss: 0.0035 - val_reg_l2_loss: 0.0935 - val_loss: 0.6113\n",
            "Epoch 4/40\n",
            "96/96 [==============================] - 76s 795ms/step - det_loss: 0.3736 - cls_loss: 0.2228 - box_loss: 0.0030 - reg_l2_loss: 0.0936 - loss: 0.4672 - learning_rate: 0.0098 - gradient_norm: 4.0088 - val_det_loss: 0.5182 - val_cls_loss: 0.3356 - val_box_loss: 0.0037 - val_reg_l2_loss: 0.0938 - val_loss: 0.6119\n",
            "Epoch 5/40\n",
            "96/96 [==============================] - 103s 1s/step - det_loss: 0.3588 - cls_loss: 0.2140 - box_loss: 0.0029 - reg_l2_loss: 0.0939 - loss: 0.4527 - learning_rate: 0.0097 - gradient_norm: 3.8002 - val_det_loss: 0.3820 - val_cls_loss: 0.2616 - val_box_loss: 0.0024 - val_reg_l2_loss: 0.0940 - val_loss: 0.4760\n",
            "Epoch 6/40\n",
            "96/96 [==============================] - 78s 816ms/step - det_loss: 0.3343 - cls_loss: 0.1998 - box_loss: 0.0027 - reg_l2_loss: 0.0941 - loss: 0.4284 - learning_rate: 0.0095 - gradient_norm: 3.4311 - val_det_loss: 0.6137 - val_cls_loss: 0.3875 - val_box_loss: 0.0045 - val_reg_l2_loss: 0.0941 - val_loss: 0.7078\n",
            "Epoch 7/40\n",
            "96/96 [==============================] - 76s 799ms/step - det_loss: 0.3242 - cls_loss: 0.1989 - box_loss: 0.0025 - reg_l2_loss: 0.0942 - loss: 0.4185 - learning_rate: 0.0093 - gradient_norm: 3.2937 - val_det_loss: 0.3927 - val_cls_loss: 0.2734 - val_box_loss: 0.0024 - val_reg_l2_loss: 0.0943 - val_loss: 0.4870\n",
            "Epoch 8/40\n",
            "96/96 [==============================] - 77s 802ms/step - det_loss: 0.3121 - cls_loss: 0.1908 - box_loss: 0.0024 - reg_l2_loss: 0.0944 - loss: 0.4065 - learning_rate: 0.0091 - gradient_norm: 3.6762 - val_det_loss: 0.4136 - val_cls_loss: 0.2434 - val_box_loss: 0.0034 - val_reg_l2_loss: 0.0944 - val_loss: 0.5081\n",
            "Epoch 9/40\n",
            "96/96 [==============================] - 77s 800ms/step - det_loss: 0.3049 - cls_loss: 0.1900 - box_loss: 0.0023 - reg_l2_loss: 0.0945 - loss: 0.3994 - learning_rate: 0.0089 - gradient_norm: 3.1390 - val_det_loss: 0.4759 - val_cls_loss: 0.2967 - val_box_loss: 0.0036 - val_reg_l2_loss: 0.0946 - val_loss: 0.5705\n",
            "Epoch 10/40\n",
            "96/96 [==============================] - 100s 1s/step - det_loss: 0.2891 - cls_loss: 0.1754 - box_loss: 0.0023 - reg_l2_loss: 0.0946 - loss: 0.3837 - learning_rate: 0.0086 - gradient_norm: 2.9902 - val_det_loss: 0.3643 - val_cls_loss: 0.2348 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.0947 - val_loss: 0.4590\n",
            "Epoch 11/40\n",
            "96/96 [==============================] - 76s 794ms/step - det_loss: 0.2807 - cls_loss: 0.1717 - box_loss: 0.0022 - reg_l2_loss: 0.0947 - loss: 0.3754 - learning_rate: 0.0083 - gradient_norm: 2.9649 - val_det_loss: 0.4587 - val_cls_loss: 0.3133 - val_box_loss: 0.0029 - val_reg_l2_loss: 0.0947 - val_loss: 0.5534\n",
            "Epoch 12/40\n",
            "96/96 [==============================] - 77s 803ms/step - det_loss: 0.2650 - cls_loss: 0.1655 - box_loss: 0.0020 - reg_l2_loss: 0.0948 - loss: 0.3598 - learning_rate: 0.0080 - gradient_norm: 2.7932 - val_det_loss: 0.2838 - val_cls_loss: 0.1553 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.0948 - val_loss: 0.3786\n",
            "Epoch 13/40\n",
            "96/96 [==============================] - 77s 803ms/step - det_loss: 0.2611 - cls_loss: 0.1636 - box_loss: 0.0019 - reg_l2_loss: 0.0948 - loss: 0.3559 - learning_rate: 0.0077 - gradient_norm: 2.8294 - val_det_loss: 0.4333 - val_cls_loss: 0.2846 - val_box_loss: 0.0030 - val_reg_l2_loss: 0.0948 - val_loss: 0.5281\n",
            "Epoch 14/40\n",
            "96/96 [==============================] - 77s 800ms/step - det_loss: 0.2516 - cls_loss: 0.1567 - box_loss: 0.0019 - reg_l2_loss: 0.0949 - loss: 0.3465 - learning_rate: 0.0073 - gradient_norm: 2.7536 - val_det_loss: 0.2790 - val_cls_loss: 0.1601 - val_box_loss: 0.0024 - val_reg_l2_loss: 0.0949 - val_loss: 0.3739\n",
            "Epoch 15/40\n",
            "96/96 [==============================] - 101s 1s/step - det_loss: 0.2601 - cls_loss: 0.1639 - box_loss: 0.0019 - reg_l2_loss: 0.0949 - loss: 0.3550 - learning_rate: 0.0070 - gradient_norm: 2.9635 - val_det_loss: 0.3126 - val_cls_loss: 0.1926 - val_box_loss: 0.0024 - val_reg_l2_loss: 0.0949 - val_loss: 0.4075\n",
            "Epoch 16/40\n",
            "96/96 [==============================] - 76s 798ms/step - det_loss: 0.2429 - cls_loss: 0.1546 - box_loss: 0.0018 - reg_l2_loss: 0.0950 - loss: 0.3379 - learning_rate: 0.0066 - gradient_norm: 3.1231 - val_det_loss: 0.3713 - val_cls_loss: 0.2238 - val_box_loss: 0.0029 - val_reg_l2_loss: 0.0950 - val_loss: 0.4663\n",
            "Epoch 17/40\n",
            "96/96 [==============================] - 76s 797ms/step - det_loss: 0.2246 - cls_loss: 0.1453 - box_loss: 0.0016 - reg_l2_loss: 0.0950 - loss: 0.3196 - learning_rate: 0.0062 - gradient_norm: 2.9005 - val_det_loss: 0.3516 - val_cls_loss: 0.2243 - val_box_loss: 0.0025 - val_reg_l2_loss: 0.0950 - val_loss: 0.4466\n",
            "Epoch 18/40\n",
            "96/96 [==============================] - 78s 815ms/step - det_loss: 0.2288 - cls_loss: 0.1456 - box_loss: 0.0017 - reg_l2_loss: 0.0950 - loss: 0.3238 - learning_rate: 0.0058 - gradient_norm: 2.5889 - val_det_loss: 0.3056 - val_cls_loss: 0.1858 - val_box_loss: 0.0024 - val_reg_l2_loss: 0.0950 - val_loss: 0.4007\n",
            "Epoch 19/40\n",
            "96/96 [==============================] - 76s 796ms/step - det_loss: 0.2208 - cls_loss: 0.1437 - box_loss: 0.0015 - reg_l2_loss: 0.0950 - loss: 0.3159 - learning_rate: 0.0054 - gradient_norm: 2.6413 - val_det_loss: 0.3763 - val_cls_loss: 0.2350 - val_box_loss: 0.0028 - val_reg_l2_loss: 0.0951 - val_loss: 0.4713\n",
            "Epoch 20/40\n",
            "96/96 [==============================] - 101s 1s/step - det_loss: 0.2202 - cls_loss: 0.1431 - box_loss: 0.0015 - reg_l2_loss: 0.0951 - loss: 0.3152 - learning_rate: 0.0050 - gradient_norm: 2.5612 - val_det_loss: 0.4311 - val_cls_loss: 0.2810 - val_box_loss: 0.0030 - val_reg_l2_loss: 0.0951 - val_loss: 0.5262\n",
            "Epoch 21/40\n",
            "96/96 [==============================] - 76s 794ms/step - det_loss: 0.2050 - cls_loss: 0.1333 - box_loss: 0.0014 - reg_l2_loss: 0.0951 - loss: 0.3001 - learning_rate: 0.0046 - gradient_norm: 2.5351 - val_det_loss: 0.4169 - val_cls_loss: 0.2747 - val_box_loss: 0.0028 - val_reg_l2_loss: 0.0951 - val_loss: 0.5119\n",
            "Epoch 22/40\n",
            "96/96 [==============================] - 77s 808ms/step - det_loss: 0.2057 - cls_loss: 0.1350 - box_loss: 0.0014 - reg_l2_loss: 0.0951 - loss: 0.3008 - learning_rate: 0.0042 - gradient_norm: 2.4809 - val_det_loss: 0.4045 - val_cls_loss: 0.2598 - val_box_loss: 0.0029 - val_reg_l2_loss: 0.0951 - val_loss: 0.4996\n",
            "Epoch 23/40\n",
            "96/96 [==============================] - 76s 797ms/step - det_loss: 0.2024 - cls_loss: 0.1344 - box_loss: 0.0014 - reg_l2_loss: 0.0951 - loss: 0.2974 - learning_rate: 0.0038 - gradient_norm: 2.4334 - val_det_loss: 0.3657 - val_cls_loss: 0.2336 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.0951 - val_loss: 0.4607\n",
            "Epoch 24/40\n",
            "96/96 [==============================] - 77s 801ms/step - det_loss: 0.1886 - cls_loss: 0.1260 - box_loss: 0.0013 - reg_l2_loss: 0.0951 - loss: 0.2836 - learning_rate: 0.0034 - gradient_norm: 2.3313 - val_det_loss: 0.3998 - val_cls_loss: 0.2615 - val_box_loss: 0.0028 - val_reg_l2_loss: 0.0950 - val_loss: 0.4948\n",
            "Epoch 25/40\n",
            "96/96 [==============================] - 100s 1s/step - det_loss: 0.1906 - cls_loss: 0.1255 - box_loss: 0.0013 - reg_l2_loss: 0.0950 - loss: 0.2857 - learning_rate: 0.0030 - gradient_norm: 2.3331 - val_det_loss: 0.4229 - val_cls_loss: 0.2771 - val_box_loss: 0.0029 - val_reg_l2_loss: 0.0950 - val_loss: 0.5179\n",
            "Epoch 26/40\n",
            "96/96 [==============================] - 78s 815ms/step - det_loss: 0.1895 - cls_loss: 0.1264 - box_loss: 0.0013 - reg_l2_loss: 0.0950 - loss: 0.2846 - learning_rate: 0.0027 - gradient_norm: 2.4034 - val_det_loss: 0.3698 - val_cls_loss: 0.2009 - val_box_loss: 0.0034 - val_reg_l2_loss: 0.0950 - val_loss: 0.4648\n",
            "Epoch 27/40\n",
            "96/96 [==============================] - 77s 803ms/step - det_loss: 0.1824 - cls_loss: 0.1226 - box_loss: 0.0012 - reg_l2_loss: 0.0950 - loss: 0.2774 - learning_rate: 0.0023 - gradient_norm: 2.3912 - val_det_loss: 0.3949 - val_cls_loss: 0.2351 - val_box_loss: 0.0032 - val_reg_l2_loss: 0.0950 - val_loss: 0.4899\n",
            "Epoch 28/40\n",
            "96/96 [==============================] - 77s 803ms/step - det_loss: 0.1828 - cls_loss: 0.1214 - box_loss: 0.0012 - reg_l2_loss: 0.0950 - loss: 0.2778 - learning_rate: 0.0020 - gradient_norm: 2.2878 - val_det_loss: 0.4032 - val_cls_loss: 0.2793 - val_box_loss: 0.0025 - val_reg_l2_loss: 0.0950 - val_loss: 0.4983\n",
            "Epoch 29/40\n",
            "96/96 [==============================] - 77s 800ms/step - det_loss: 0.1808 - cls_loss: 0.1222 - box_loss: 0.0012 - reg_l2_loss: 0.0950 - loss: 0.2758 - learning_rate: 0.0017 - gradient_norm: 2.2765 - val_det_loss: 0.4220 - val_cls_loss: 0.2957 - val_box_loss: 0.0025 - val_reg_l2_loss: 0.0950 - val_loss: 0.5170\n",
            "Epoch 30/40\n",
            "96/96 [==============================] - 100s 1s/step - det_loss: 0.1704 - cls_loss: 0.1177 - box_loss: 0.0011 - reg_l2_loss: 0.0950 - loss: 0.2654 - learning_rate: 0.0014 - gradient_norm: 2.2415 - val_det_loss: 0.4136 - val_cls_loss: 0.2918 - val_box_loss: 0.0024 - val_reg_l2_loss: 0.0950 - val_loss: 0.5086\n",
            "Epoch 31/40\n",
            "96/96 [==============================] - 76s 798ms/step - det_loss: 0.1601 - cls_loss: 0.1103 - box_loss: 9.9469e-04 - reg_l2_loss: 0.0950 - loss: 0.2551 - learning_rate: 0.0011 - gradient_norm: 2.1896 - val_det_loss: 0.3841 - val_cls_loss: 0.2653 - val_box_loss: 0.0024 - val_reg_l2_loss: 0.0950 - val_loss: 0.4791\n",
            "Epoch 32/40\n",
            "96/96 [==============================] - 76s 799ms/step - det_loss: 0.1655 - cls_loss: 0.1128 - box_loss: 0.0011 - reg_l2_loss: 0.0950 - loss: 0.2605 - learning_rate: 8.8624e-04 - gradient_norm: 2.2352 - val_det_loss: 0.4184 - val_cls_loss: 0.2901 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.0950 - val_loss: 0.5134\n",
            "Epoch 33/40\n",
            "96/96 [==============================] - 77s 803ms/step - det_loss: 0.1711 - cls_loss: 0.1165 - box_loss: 0.0011 - reg_l2_loss: 0.0950 - loss: 0.2661 - learning_rate: 6.7109e-04 - gradient_norm: 2.4039 - val_det_loss: 0.4087 - val_cls_loss: 0.2772 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.0950 - val_loss: 0.5037\n",
            "Epoch 34/40\n",
            "96/96 [==============================] - 78s 810ms/step - det_loss: 0.1613 - cls_loss: 0.1093 - box_loss: 0.0010 - reg_l2_loss: 0.0950 - loss: 0.2563 - learning_rate: 4.8401e-04 - gradient_norm: 2.2553 - val_det_loss: 0.4214 - val_cls_loss: 0.2819 - val_box_loss: 0.0028 - val_reg_l2_loss: 0.0950 - val_loss: 0.5164\n",
            "Epoch 35/40\n",
            "96/96 [==============================] - 99s 1s/step - det_loss: 0.1666 - cls_loss: 0.1132 - box_loss: 0.0011 - reg_l2_loss: 0.0950 - loss: 0.2616 - learning_rate: 3.2622e-04 - gradient_norm: 2.3099 - val_det_loss: 0.4130 - val_cls_loss: 0.2780 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.0950 - val_loss: 0.5079\n",
            "Epoch 36/40\n",
            "96/96 [==============================] - 77s 800ms/step - det_loss: 0.1631 - cls_loss: 0.1115 - box_loss: 0.0010 - reg_l2_loss: 0.0950 - loss: 0.2581 - learning_rate: 1.9875e-04 - gradient_norm: 2.2288 - val_det_loss: 0.4164 - val_cls_loss: 0.2825 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.0950 - val_loss: 0.5114\n",
            "Epoch 37/40\n",
            "96/96 [==============================] - 77s 801ms/step - det_loss: 0.1696 - cls_loss: 0.1149 - box_loss: 0.0011 - reg_l2_loss: 0.0950 - loss: 0.2646 - learning_rate: 1.0241e-04 - gradient_norm: 2.4000 - val_det_loss: 0.4179 - val_cls_loss: 0.2826 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.0950 - val_loss: 0.5129\n",
            "Epoch 38/40\n",
            "96/96 [==============================] - 78s 812ms/step - det_loss: 0.1615 - cls_loss: 0.1095 - box_loss: 0.0010 - reg_l2_loss: 0.0950 - loss: 0.2565 - learning_rate: 3.7829e-05 - gradient_norm: 2.0696 - val_det_loss: 0.4143 - val_cls_loss: 0.2808 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.0950 - val_loss: 0.5093\n",
            "Epoch 39/40\n",
            "96/96 [==============================] - 77s 801ms/step - det_loss: 0.1602 - cls_loss: 0.1087 - box_loss: 0.0010 - reg_l2_loss: 0.0950 - loss: 0.2552 - learning_rate: 5.4338e-06 - gradient_norm: 2.1609 - val_det_loss: 0.4149 - val_cls_loss: 0.2805 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.0950 - val_loss: 0.5098\n",
            "Epoch 40/40\n",
            "96/96 [==============================] - 99s 1s/step - det_loss: 0.1668 - cls_loss: 0.1130 - box_loss: 0.0011 - reg_l2_loss: 0.0950 - loss: 0.2617 - learning_rate: 5.4304e-06 - gradient_norm: 2.2984 - val_det_loss: 0.4175 - val_cls_loss: 0.2826 - val_box_loss: 0.0027 - val_reg_l2_loss: 0.0950 - val_loss: 0.5125\n"
          ]
        }
      ],
      "source": [
        "model = object_detector.create(train_data, model_spec=spec, epochs=40, batch_size=8, train_whole_model=True, validation_data=validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BzCHLWJ6h7q"
      },
      "source": [
        "**Step 4. Evaluate the model with the test data.**\n",
        "\n",
        "After training the object detection model using the images in the training dataset, use the remaining 25 images in the test dataset to evaluate how the model performs against new data it has never seen before.\n",
        "\n",
        "As the default batch size is 64, it will take 1 step to go through the 7 images in the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xmnl6Yy7ARn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7693cab4-71fb-40a2-9bbf-d89c3ed0a82f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 27s 2s/step\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AP': 0.62488,\n",
              " 'AP50': 0.94523716,\n",
              " 'AP75': 0.73806816,\n",
              " 'AP_/drone': 0.62488,\n",
              " 'APl': 0.6724523,\n",
              " 'APm': 0.44989026,\n",
              " 'APs': 0.12042438,\n",
              " 'ARl': 0.76971155,\n",
              " 'ARm': 0.62222224,\n",
              " 'ARmax1': 0.649904,\n",
              " 'ARmax10': 0.72380036,\n",
              " 'ARmax100': 0.7353167,\n",
              " 'ARs': 0.46}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "model.evaluate(validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgCDMe0e6jlT"
      },
      "source": [
        "**Step 5.  Export as a TensorFlow Lite model.**\n",
        "\n",
        "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is full integer quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm_UULdW7A9T"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='/content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UygYErfCD5m3"
      },
      "source": [
        "**Step 6.  Evaluate the TensorFlow Lite model.**\n",
        "\n",
        "Several factors can affect the model accuracy when exporting to TFLite:\n",
        "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop. \n",
        "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
        "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
        "\n",
        "Therefore you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHYDWcljr6jq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "dcdd0141-91a1-43af-b1a9-fd2868b2281b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-bb4b22df439f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_tflite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/object_detector.py\u001b[0m in \u001b[0;36mevaluate_tflite\u001b[0;34m(self, tflite_filepath, data)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     return self.model_spec.evaluate_tflite(tflite_filepath, ds, len(data),\n\u001b[0;32m--> 156\u001b[0;31m                                            data.annotations_json_file)\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_export_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_model_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/core/task/model_spec/object_detector_spec.py\u001b[0m in \u001b[0;36mevaluate_tflite\u001b[0;34m(self, tflite_filepath, dataset, steps, json_file)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m     \u001b[0mlite_runner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_tflite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLiteRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_network\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m     \u001b[0mprogbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProgbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_examples/lite/model_maker/third_party/efficientdet/keras/eval_tflite.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tflite_model_path, only_network)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mNMS\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpreter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocate_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# Get input and output tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors)\u001b[0m\n\u001b[1;32m    456\u001b[0m           _interpreter_wrapper.CreateWrapperFromFile(\n\u001b[1;32m    457\u001b[0m               \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_resolver_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_op_registerers_by_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m               custom_op_registerers_by_func, experimental_preserve_all_tensors))\n\u001b[0m\u001b[1;32m    459\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Failed to open {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Could not open '/content/model'."
          ]
        }
      ],
      "source": [
        "model.evaluate_tflite('/content/model', validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1egWlephdND"
      },
      "source": [
        "## (Optional) Test the TFLite model on your image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python-headless==4.1.2.30"
      ],
      "metadata": {
        "id": "_K-Clvokdkkc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2b7483-9e48-4280-e461-75464c42bc79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python-headless==4.1.2.30\n",
            "  Downloading opencv_python_headless-4.1.2.30-cp37-cp37m-manylinux1_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 787 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless==4.1.2.30) (1.21.5)\n",
            "Installing collected packages: opencv-python-headless\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.5.5.62\n",
            "    Uninstalling opencv-python-headless-4.5.5.62:\n",
            "      Successfully uninstalled opencv-python-headless-4.5.5.62\n",
            "Successfully installed opencv-python-headless-4.1.2.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QL_q7m3hjfA"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "model_path = '/content/model_ef0_ep30_bs8.tflite'\n",
        "\n",
        "# Load the labels into a list\n",
        "classes = ['drone'] * model.model_spec.config.num_classes\n",
        "label_map = model.model_spec.config.label_map\n",
        "for label_id, label_name in label_map.as_dict().items():\n",
        "  classes[label_id-1] = label_name\n",
        "\n",
        "# Define a list of colors for visualization\n",
        "COLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)\n",
        "\n",
        "def preprocess_image(image_path, input_size):\n",
        "  \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n",
        "  img = tf.io.read_file(image_path)\n",
        "  img = tf.io.decode_image(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.uint8)\n",
        "  original_image = img\n",
        "  resized_img = tf.image.resize(img, input_size)\n",
        "  resized_img = resized_img[tf.newaxis, :]\n",
        "  return resized_img, original_image\n",
        "\n",
        "\n",
        "def set_input_tensor(interpreter, image):\n",
        "  \"\"\"Set the input tensor.\"\"\"\n",
        "  tensor_index = interpreter.get_input_details()[0]['index']\n",
        "  input_tensor = interpreter.tensor(tensor_index)()[0]\n",
        "  input_tensor[:, :] = image\n",
        "\n",
        "\n",
        "def get_output_tensor(interpreter, index):\n",
        "  \"\"\"Retur the output tensor at the given index.\"\"\"\n",
        "  output_details = interpreter.get_output_details()[index]\n",
        "  tensor = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
        "  return tensor\n",
        "\n",
        "\n",
        "def detect_objects(interpreter, image, threshold):\n",
        "  \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
        "  # Feed the input image to the model\n",
        "  set_input_tensor(interpreter, image)\n",
        "  interpreter.invoke()\n",
        "\n",
        "  # Get all outputs from the model\n",
        "  scores = get_output_tensor(interpreter, 0)\n",
        "  boxes = get_output_tensor(interpreter, 1)\n",
        "  count = int(get_output_tensor(interpreter, 2))\n",
        "  classes = get_output_tensor(interpreter, 3)\n",
        "\n",
        "  results = []\n",
        "  for i in range(count):\n",
        "    if scores[i] >= threshold:\n",
        "      result = {\n",
        "        'bounding_box': boxes[i],\n",
        "        'class_id': classes[i],\n",
        "        'score': scores[i]\n",
        "      }\n",
        "      results.append(result)\n",
        "  return results\n",
        "\n",
        "\n",
        "def run_odt_and_draw_results(image_path, interpreter, threshold=0.5):\n",
        "  \"\"\"Run object detection on the input image and draw the detection results\"\"\"\n",
        "  # Load the input shape required by the model\n",
        "  _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n",
        "\n",
        "  # Load the input image and preprocess it\n",
        "  preprocessed_image, original_image = preprocess_image(\n",
        "      image_path, \n",
        "      (input_height, input_width)\n",
        "    )\n",
        "\n",
        "  # Run object detection on the input image\n",
        "  results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
        "\n",
        "  # Plot the detection results on the input image\n",
        "  original_image_np = original_image.numpy().astype(np.uint8)\n",
        "  for obj in results:\n",
        "    # Convert the object bounding box from relative coordinates to absolute \n",
        "    # coordinates based on the original image resolution\n",
        "    ymin, xmin, ymax, xmax = obj['bounding_box']\n",
        "    xmin = int(xmin * original_image_np.shape[1])\n",
        "    xmax = int(xmax * original_image_np.shape[1])\n",
        "    ymin = int(ymin * original_image_np.shape[0])\n",
        "    ymax = int(ymax * original_image_np.shape[0])\n",
        "\n",
        "    # Find the class index of the current object\n",
        "    class_id = int(obj['class_id'])\n",
        "\n",
        "    # Draw the bounding box and label on the image\n",
        "    color = [int(c) for c in COLORS[class_id]]\n",
        "    cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\n",
        "    # Make adjustments to make the label visible for all objects\n",
        "    y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
        "    label = \"{}: {:.0f}%\".format(classes[class_id], obj['score'] * 100)\n",
        "    cv2.putText(original_image_np, label, (xmin, y),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "  # Return the final image\n",
        "  original_uint8 = original_image_np.astype(np.uint8)\n",
        "  return original_uint8"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_IMAGE_URL = \"/content/WhatsApp Image 2022-02-10 at 6.01.36 PM.jpeg\"\n",
        "DETECTION_THRESHOLD = 0.5 \n",
        "\n",
        "# Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Run inference and draw detection result on the local copy of the original file\n",
        "detection_result_image = run_odt_and_draw_results(\n",
        "    INPUT_IMAGE_URL, \n",
        "    interpreter, \n",
        "    threshold=DETECTION_THRESHOLD\n",
        ")\n",
        "\n",
        "# Show the detection result\n",
        "Image.fromarray(detection_result_image)"
      ],
      "metadata": {
        "id": "2j1blG2h4Jze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                                                                                                                      niyaskhan1011"
      ],
      "metadata": {
        "id": "BpuhX7Cs4_2k"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Train custom object detector with TFLite Model Maker",
      "provenance": []
    },
    "interpreter": {
      "hash": "3f06a60cda354d81c2225d2767ee5b44cb4579f2789781878c56ddc9e6e490bb"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('detectron2': conda)",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}